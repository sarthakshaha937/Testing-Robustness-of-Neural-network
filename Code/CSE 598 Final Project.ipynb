{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bysd-eb3XtYf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # Displays a progress bar\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and train, val, test splits\n",
        "!rm -rf Fashion* \n",
        "print(\"Loading datasets...\")\n",
        "FASHION_transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
        "])\n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_train = Subset(FASHION_trainval, range(50000))\n",
        "FASHION_val = Subset(FASHION_trainval, range(50000,60000))\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "zJziYS0vXyVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "trainloader = DataLoader(FASHION_train, batch_size=64, shuffle=True)\n",
        "valloader = DataLoader(FASHION_val, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(FASHION_test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "pXTivOoOX0Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        out1 = 24\n",
        "        out2 = 64\n",
        "        kernel1 = (5,5)\n",
        "        kernel2 = (3,3)\n",
        "\n",
        "        kernel_pool = (2,2)\n",
        "        stride_pool = (2,2)\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out1, kernel_size=kernel1, padding=0)  # size = 28-kernel1+1\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=kernel_pool, stride=stride_pool)        # size = ( (28-kernel1+1)-kernel_pool / stride) + 1\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out1, out_channels=out2, kernel_size=kernel2) # size = ( (28-kernel1+1)-kernel_pool / stride) + 2 - kernel2\n",
        "        \n",
        "        temp = (((28-kernel1[0]+1)-kernel_pool[0]) // stride_pool[0]) + 2 - kernel2[0]\n",
        "        h_fc3 = int((((temp)-kernel_pool[0]) // stride_pool[0]) + 1)\n",
        "        \n",
        "        self.fc3 = nn.Linear(h_fc3*h_fc3*out2, 256)\n",
        "        self.fc4 = nn.Linear(256, 64)\n",
        "        self.fc5 = nn.Linear(64, 24)\n",
        "        self.drop1 = nn.Dropout2d(0.2)\n",
        "        self.drop2 = nn.Dropout1d(0.2)\n",
        "        self.fc6 = nn.Linear(24, 10)\n",
        "\n",
        "        self.batchNorm1 = nn.BatchNorm2d(out1)\n",
        "        self.batchNorm2 = nn.BatchNorm2d(out2)\n",
        "        self.batchNorm3 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.cnn1(x) \n",
        "        x = self.pool1(x)\n",
        "        x = self.batchNorm1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.drop1(x)\n",
        "        \n",
        "\n",
        "        x = self.cnn2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.batchNorm2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        \n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc3(x)\n",
        "        x = self.batchNorm3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        self.drop2(x)\n",
        "\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc4(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc5(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc6(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "EIGgSwDSX16e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n",
        "model = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "optimizer = optim.Adam(model.parameters(), lr=2*1e-3, weight_decay=7*1e-4) # Specify optimizer and assign trainable parameters to it, weight_decay is L2 regularization strength"
      ],
      "metadata": {
        "id": "95jkFVbbX-E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_acc_epoch = []\n",
        "training_loss_epoch = []\n",
        "def train(model=None, loader=None, num_epoch = 10, model_name=\"model1.pt\", validation=valloader): # Train the model\n",
        "    valid_acc_epoch = []\n",
        "    training_loss_epoch = []\n",
        "    print(\"Start training...\")\n",
        "    model.train() # Set the model to training mode\n",
        "    max_valid_acc = 0\n",
        "    for i in range(num_epoch):\n",
        "        running_loss = []\n",
        "        for batch, label in tqdm(loader):\n",
        "            batch = batch.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad() # Clear gradients from the previous iteration\n",
        "            pred = model(batch) # This will call Network.forward() that you implement\n",
        "            loss = criterion(pred, label) # Calculate the loss\n",
        "            running_loss.append(loss.item())\n",
        "            loss.backward() # Backprop gradients to all tensors in the network\n",
        "            optimizer.step() # Update trainable weights\n",
        "        print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
        "        valid_acc = evaluate(model, validation)\n",
        "        valid_acc_epoch.append((i, valid_acc))\n",
        "        training_loss_epoch.append((i, float(np.mean(running_loss))))\n",
        "        if valid_acc > max_valid_acc:\n",
        "          max_valid_acc = valid_acc\n",
        "          torch.save(model.state_dict(), model_name)\n",
        "        model.train()\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "6kN29CXQYC8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
        "        for batch, label in tqdm(loader):\n",
        "            batch = batch.to(device)\n",
        "            label = label.to(device)\n",
        "            pred = model(batch)\n",
        "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
        "    acc = correct/len(loader.dataset)\n",
        "    print(\"Evaluation accuracy: {}\".format(acc))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "DGfq44JGYH6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, trainloader, 15)"
      ],
      "metadata": {
        "id": "EFYfx8IiYIz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model1.pt'))\n",
        "print(\"Evaluate on validation set...\")\n",
        "evaluate(model, valloader)\n",
        "print(\"Evaluate on test set\")\n",
        "evaluate(model, testloader)"
      ],
      "metadata": {
        "id": "aLPChkhVYKSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nsmKngQAYa0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_adv_images = []\n",
        "def test_pgd(model, device, testing, epsilon, alpha, steps):\n",
        "    correct_predictions = 0\n",
        "    adversarial = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for index, (batch, label) in enumerate(testing):\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        \n",
        "\n",
        "        for _ in range(steps):\n",
        "          batch = batch.detach()\n",
        "          batch.requires_grad = True\n",
        "          output = model(batch)\n",
        "          initial_prediction = output.max(1, keepdim=True)[1]\n",
        "\n",
        "          loss = loss_function(output, label)\n",
        "        \n",
        "          # Zero all existing gradients\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Calculate gradients of model in backward pass\n",
        "          loss.backward()\n",
        "          # Collect batchgrad\n",
        "          batch_grad = batch.grad.data\n",
        "\n",
        "          # FGSM Attack\n",
        "          sign_batch_grad = batch_grad.sign()\n",
        "          batch = batch + alpha*sign_batch_grad\n",
        "          # Adding clipping to maintain [0,1] range\n",
        "          batch = torch.clamp(batch, 0, 1)\n",
        "          batch = torch.where(batch > batch + epsilon, batch + epsilon, batch)\n",
        "          batch = torch.where(batch < batch - epsilon, batch - epsilon, batch)\n",
        "\n",
        "        # Re-classify the perturbed batch\n",
        "        output = model(batch)\n",
        "        # Randomly select 10 images\n",
        "        if steps == 10 and len(random_adv_images) < 10:\n",
        "          random_adv_images.append(batch[random.randint(0,10)])\n",
        "\n",
        "        perturbed_prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        perturbed_prediction_arr = torch.reshape(perturbed_prediction, (perturbed_prediction.size(dim=0), )).numpy()\n",
        "        label_arr = label.numpy()\n",
        "        for index in range(label.size(dim=0)):\n",
        "          if perturbed_prediction_arr[index] == label_arr[index]:\n",
        "              correct_predictions = correct_predictions + 1\n",
        "\n",
        "    accuracy = correct_predictions / (len(testing)*64) # multiplying by 64 due to batch size\n",
        "    print(\"accuracy after PGD attack for alpha = {}, epsilon = {}, attack steps = {} is {} or {}%\".format(alpha, epsilon, attack_steps, accuracy, accuracy*100))\n",
        "\n",
        "\n",
        "for alpha in (0.01, 0.02):\n",
        "  for attack_steps in (1, 2, 5, 10):\n",
        "    test_pgd(model, device, testloader, 25/255, alpha, attack_steps)"
      ],
      "metadata": {
        "id": "JAGcJanPYbV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "train_valloader = DataLoader(FASHION_trainval, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "CaL-3rlZYeNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_train_pgd = []\n",
        "# get pgd adversarial images for training data\n",
        "def train_with_pgd(model, device, training, epsilon, alpha, steps):\n",
        "    # Loop over all examples in training set\n",
        "    for index, (batch, label) in enumerate(training):\n",
        "        if index == 2:\n",
        "          break\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        \n",
        "\n",
        "        for _ in range(steps):\n",
        "          batch = batch.detach()\n",
        "          batch.requires_grad = True\n",
        "          output = model(batch)\n",
        "          initial_prediction = output.max(1, keepdim=True)[1]\n",
        "\n",
        "          loss = loss_function(output, label)\n",
        "        \n",
        "          # Zero all existing gradients\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Calculate gradients of model in backward pass\n",
        "          loss.backward()\n",
        "          # Collect batchgrad\n",
        "          batch_grad = batch.grad.data\n",
        "\n",
        "          # FGSM Attack\n",
        "          sign_batch_grad = batch_grad.sign()\n",
        "          batch = batch + alpha*sign_batch_grad\n",
        "          # Adding clipping to maintain [0,1] range\n",
        "          batch = torch.clamp(batch, 0, 1)\n",
        "          batch = torch.where(batch > batch + epsilon, batch + epsilon, batch)\n",
        "          batch = torch.where(batch < batch - epsilon, batch - epsilon, batch)\n",
        "\n",
        "        # Re-classify the perturbed batch\n",
        "        # print(len(batch), len(batch[0]), batch)\n",
        "\n",
        "        label_arr = label.numpy()\n",
        "        for index, pgd_image in enumerate(batch):\n",
        "          adv_train_pgd.append((pgd_image, label_arr[index]))\n",
        "\n",
        "for alpha in (0.01, 0.02):\n",
        "  for attack_steps in (1, 2, 5, 10):\n",
        "    train_with_pgd(model, device, train_valloader, 25/255, alpha, attack_steps)"
      ],
      "metadata": {
        "id": "awfmbuCwYiit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "\n",
        "FASHION_trainval_pgd = torch.utils.data.ConcatDataset([FASHION_trainval, adv_train_pgd])\n",
        "train_len = int(0.8*len(FASHION_trainval_pgd))\n",
        "FASHION_train_pgd = Subset(FASHION_trainval_pgd, range(train_len))\n",
        "FASHION_val_pgd = Subset(FASHION_trainval_pgd, range(train_len, len(FASHION_trainval_pgd)))\n",
        "\n",
        "trainloader_pgd = DataLoader(FASHION_train_pgd, batch_size=64, shuffle=True)\n",
        "valloader_pgd = DataLoader(FASHION_val_pgd, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "model_pgd = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "optimizer = optim.Adam(model_pgd.parameters(), lr=2*1e-3, weight_decay=7*1e-4)"
      ],
      "metadata": {
        "id": "u-bYBOYdYmAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_pgd, trainloader_pgd, 10, \"model_pgd\", valloader_pgd)"
      ],
      "metadata": {
        "id": "WEFdeuddYotS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run adversarial test images on model trained with pgd images\n",
        "for alpha in (0.01, 0.02):\n",
        "  for attack_steps in (1, 2, 5, 10):\n",
        "    test_pgd(model_pgd, device, testloader, 25/255, alpha, attack_steps)"
      ],
      "metadata": {
        "id": "xgtOtnbmYqpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "uap"
      ],
      "metadata": {
        "id": "MyaXfxHcYzfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "testloader = DataLoader(FASHION_test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "BY-BhH57YwGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get pgd adversarial images for training data\n",
        "def test_with_uap(model, device, testing, epsilon, alpha, steps):\n",
        "    correct_predictions = 0\n",
        "    # Loop over all examples in training set\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    for step in range(steps):\n",
        "      uap = torch.zeros(64, 1, 28, 28).to(device)\n",
        "      uap.requires_grad = True\n",
        "      optimizer = optim.SGD([uap], lr=alpha)\n",
        "      for index, (batch, label) in enumerate(testing):\n",
        "          if len(batch) != 64: # skipping last batch\n",
        "            continue\n",
        "          uap = uap.detach()\n",
        "          uap = uap.to(device)\n",
        "          batch = batch.to(device)\n",
        "          label = label.to(device)\n",
        "          batch = batch.detach()\n",
        "          batch.requires_grad = True\n",
        "\n",
        "          adv_images = torch.clamp(batch + uap, 0, 1)\n",
        "\n",
        "          output = model(adv_images)\n",
        "          initial_prediction = output.max(1, keepdim=True)[1]\n",
        "\n",
        "          loss = loss_function(output, label)\n",
        "        \n",
        "          # Zero all existing gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Calculate gradients of model in backward pass\n",
        "          loss.backward()\n",
        "          uap = uap + alpha*batch.grad.data.sign()\n",
        "          # Adding clipping to maintain [0,1] range\n",
        "          uap = torch.clamp(uap, 0, 1)\n",
        "          uap = torch.where(uap > uap + epsilon, uap + epsilon, uap)\n",
        "          uap = torch.where(uap < uap - epsilon, uap - epsilon, uap)\n",
        "\n",
        "\n",
        "\n",
        "      for index, (batch, label) in enumerate(testing):\n",
        "          batch = batch.to(device)\n",
        "          label = label.to(device)\n",
        "          batch = batch.detach()\n",
        "          batch.requires_grad = True\n",
        "          if len(batch) != 64:\n",
        "            continue\n",
        "          batch = batch + uap\n",
        "          batch = torch.clamp(batch, 0, 1)\n",
        "          batch = torch.where(batch > batch + epsilon, batch + epsilon, batch)\n",
        "          batch = torch.where(batch < batch - epsilon, batch - epsilon, batch)\n",
        "\n",
        "          if step == steps-1:\n",
        "            output = model(batch)\n",
        "            perturbed_prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            perturbed_prediction_arr = torch.reshape(perturbed_prediction, (perturbed_prediction.size(dim=0), )).numpy()\n",
        "            label_arr = label.numpy()\n",
        "\n",
        "            for index in range(label.size(dim=0)):\n",
        "              if perturbed_prediction_arr[index] == label_arr[index]:\n",
        "                  correct_predictions = correct_predictions + 1\n",
        "\n",
        "    accuracy = correct_predictions / (len(testing)*64) # multiplying by 64 due to batch size\n",
        "    print(\"accuracy after UAP attack for alpha = {}, epsilon = {}, iterations =  is {} or {}%\".format(alpha, epsilon, accuracy, accuracy*100))\n",
        "\n",
        "for alpha in (0.01, 0.02):\n",
        "  # for attack_steps in (1):\n",
        "  test_with_uap(model, device, testloader, 25/255, alpha, 1)"
      ],
      "metadata": {
        "id": "tdykf50QY095"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "\n",
        "train_valloader = DataLoader(FASHION_trainval, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "_AFU8IN1Y2r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_train_uap = []\n",
        "def train_with_uap(model, device, testing, epsilon, alpha, steps):\n",
        "    # Loop over all examples in training set\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    for step in range(steps):\n",
        "      uap = torch.zeros(64, 1, 28, 28).to(device)\n",
        "      uap.requires_grad = True\n",
        "      optimizer = optim.SGD([uap], lr=alpha)\n",
        "      for index, (batch, label) in enumerate(testing):\n",
        "          if len(batch) != 64: # skipping last batch\n",
        "            continue\n",
        "          uap = uap.detach()\n",
        "          uap = uap.to(device)\n",
        "          batch = batch.to(device)\n",
        "          label = label.to(device)\n",
        "          batch = batch.detach()\n",
        "          batch.requires_grad = True\n",
        "\n",
        "          adv_images = torch.clamp(batch + uap, 0, 1)\n",
        "\n",
        "          output = model(adv_images)\n",
        "          initial_prediction = output.max(1, keepdim=True)[1]\n",
        "\n",
        "          loss = loss_function(output, label)\n",
        "        \n",
        "          # Zero all existing gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Calculate gradients of model in backward pass\n",
        "          loss.backward()\n",
        "          uap = uap + alpha*batch.grad.data.sign()\n",
        "          # Adding clipping to maintain [0,1] range\n",
        "          uap = torch.clamp(uap, 0, 1)\n",
        "          uap = torch.where(uap > uap + epsilon, uap + epsilon, uap)\n",
        "          uap = torch.where(uap < uap - epsilon, uap - epsilon, uap)\n",
        "\n",
        "\n",
        "\n",
        "      for index, (batch, label) in enumerate(testing):\n",
        "          if index == 4:\n",
        "            break\n",
        "          batch = batch.to(device)\n",
        "          label = label.to(device)\n",
        "          batch = batch.detach()\n",
        "          batch.requires_grad = True\n",
        "          if len(batch) != 64:\n",
        "            continue\n",
        "          batch = batch + uap\n",
        "          batch = torch.clamp(batch, 0, 1)\n",
        "          batch = torch.where(batch > batch + epsilon, batch + epsilon, batch)\n",
        "          batch = torch.where(batch < batch - epsilon, batch - epsilon, batch)\n",
        "          label_arr = label.numpy()\n",
        "          for index, uap_image in enumerate(batch):\n",
        "            adv_train_uap.append((uap_image, label_arr[index]))\n",
        "\n",
        "for alpha in (0.01, 0.02):\n",
        "  # for attack_steps in (1):\n",
        "  train_with_uap(model, device, train_valloader, 25/255, alpha, 1)"
      ],
      "metadata": {
        "id": "lLLe5F0rZAVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "\n",
        "FASHION_trainval_uap = torch.utils.data.ConcatDataset([FASHION_trainval, adv_train_uap])\n",
        "train_len = int(0.8*len(FASHION_trainval_uap))\n",
        "FASHION_train_uap = Subset(FASHION_trainval_uap, range(train_len))\n",
        "FASHION_val_uap = Subset(FASHION_trainval_uap, range(train_len, len(FASHION_trainval_uap)))\n",
        "\n",
        "trainloader_uap = DataLoader(FASHION_train_uap, batch_size=64, shuffle=True)\n",
        "valloader_uap = DataLoader(FASHION_val_uap, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "model_uap = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "optimizer = optim.Adam(model_uap.parameters(), lr=2*1e-3, weight_decay=7*1e-4)\n",
        "\n",
        "len(FASHION_trainval_uap)"
      ],
      "metadata": {
        "id": "wwBrSh4OZCPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_uap, trainloader_uap, 10, \"model_uap\", valloader_uap)"
      ],
      "metadata": {
        "id": "pOj6eG2OZEUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in (0.01, 0.02):\n",
        "  test_with_uap(model_uap, device, testloader, 25/255, alpha, 1)"
      ],
      "metadata": {
        "id": "jhhr_8DoZIY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "deepfool"
      ],
      "metadata": {
        "id": "7nfICDpZZQzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchattacks"
      ],
      "metadata": {
        "id": "zntRy9ETZJka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, '..')\n",
        "import torchattacks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # Displays a progress bar\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "vFTLpFRUaQM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and train, val, test splits\n",
        "!rm -rf Fashion* \n",
        "print(\"Loading datasets...\")\n",
        "FASHION_transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
        "])\n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_train = Subset(FASHION_trainval, range(50000))\n",
        "FASHION_val = Subset(FASHION_trainval, range(50000,60000))\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "lzK6g4mPaSic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "trainloader = DataLoader(FASHION_train, batch_size=64,shuffle=True)\n",
        "valloader = DataLoader(FASHION_val, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(FASHION_test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "IeH3TMWbaZk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images1, labels1 = next(iter(trainloader))"
      ],
      "metadata": {
        "id": "n1GT1jVFab0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network().to(device)\n",
        "model.load_state_dict(torch.load(\"model1.pt\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "1cTChyVraeJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import DeepFool, Square\n",
        "from utils import imshow, get_pred"
      ],
      "metadata": {
        "id": "zsD0-262ayAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atk = DeepFool(model)\n",
        "atk2 = Square(model)"
      ],
      "metadata": {
        "id": "ncwlN_oWa4e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_DeepFool(model, device, testing, attack):\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for index, (batch, label) in enumerate(testing):\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      # if index == 10: # testing\n",
        "      #   break\n",
        "      adv_image = attack(batch, label)\n",
        "      output = model(adv_image)\n",
        "      perturbed_prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "      perturbed_prediction_arr = torch.reshape(perturbed_prediction, (perturbed_prediction.size(dim=0), ))\n",
        "      label_arr = label\n",
        "      for index in range(label.size(dim=0)):\n",
        "        if perturbed_prediction_arr[index] == label_arr[index]:\n",
        "          correct_predictions = correct_predictions + 1\n",
        "  accuracy = correct_predictions / (len(testing)*64) # multiplying by 64 due to batch size\n",
        "  print(\"accuracy after DeepFool attack is {} or {}%\".format(accuracy, accuracy*100))\n",
        "test_with_DeepFool(model, device, testloader, atk)"
      ],
      "metadata": {
        "id": "YNMWcexha8ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_images = atk(images1, labels1)\n",
        "idx = 21\n",
        "pre = get_pred(model, adv_images[idx:idx+1], device)\n",
        "imshow(adv_images[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels1[idx], pre))"
      ],
      "metadata": {
        "id": "XwkMNO8XbDQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "\n",
        "train_valloader = DataLoader(FASHION_trainval, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "YKW22mL1bFgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_train_DeepFool = []\n",
        "\n",
        "def train_with_DeepFool(model, device, training, attack):\n",
        "    # Loop over all examples in training set\n",
        "    for index, (batch, label) in enumerate(training):\n",
        "        if index == 8:\n",
        "          break\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        adv_image = attack(batch, label)\n",
        "        for index, DeepFool_image in enumerate(adv_image):\n",
        "          adv_train_DeepFool.append((DeepFool_image, label[index]))\n",
        "train_with_DeepFool(model, device, train_valloader, atk)  "
      ],
      "metadata": {
        "id": "S2E5cIFgbHf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "FASHION_trainval_DeepFool = torch.utils.data.ConcatDataset([FASHION_trainval, adv_train_DeepFool])\n",
        "train_len = int(0.8*len(FASHION_trainval_DeepFool))\n",
        "FASHION_train_DeepFool = Subset(FASHION_trainval_DeepFool, range(train_len))\n",
        "FASHION_val_DeepFool = Subset(FASHION_trainval_DeepFool, range(train_len, len(FASHION_trainval_DeepFool)))\n",
        "\n",
        "trainloader_DeepFool = DataLoader(FASHION_train_DeepFool, batch_size=64, shuffle=True)\n",
        "valloader_DeepFool = DataLoader(FASHION_val_DeepFool, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "model_DeepFool = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "optimizer = optim.Adam(model_DeepFool.parameters(), lr=2*1e-3, weight_decay=7*1e-4)"
      ],
      "metadata": {
        "id": "8eazvNF8bMdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS THROWS ERROR"
      ],
      "metadata": {
        "id": "HLTAxVXXbSMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train(model_DeepFool, trainloader_DeepFool, 10, \"model_DeepFool\", valloader_DeepFool)"
      ],
      "metadata": {
        "id": "o4Inlvk6bM77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Square Attack"
      ],
      "metadata": {
        "id": "EzP49AuNbwU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and train, val, test splits\n",
        "!rm -rf Fashion* \n",
        "print(\"Loading datasets...\")\n",
        "FASHION_transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
        "])\n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_train = Subset(FASHION_trainval, range(50000))\n",
        "FASHION_val = Subset(FASHION_trainval, range(50000,60000))\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "o8pCt41dcLIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "trainloader = DataLoader(FASHION_train, batch_size=64,shuffle=True)\n",
        "valloader = DataLoader(FASHION_val, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(FASHION_test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "CYkPUI-McO2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atk3 = Square(model)"
      ],
      "metadata": {
        "id": "CAOqZ0SqcQrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_Square(model, device, testing, attack):\n",
        "  correct_predictions = 0\n",
        "\n",
        "\n",
        "  for index, (batch, label) in enumerate(testing):\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      # if index == 10: # testing\n",
        "      #   break\n",
        "      print(\"Index = \",index)\n",
        "      adv_image = attack(batch, label)\n",
        "      output = model(adv_image)\n",
        "      perturbed_prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "      perturbed_prediction_arr = torch.reshape(perturbed_prediction, (perturbed_prediction.size(dim=0), ))\n",
        "      label_arr = label\n",
        "      for index in range(label.size(dim=0)):\n",
        "        if perturbed_prediction_arr[index] == label_arr[index]:\n",
        "          correct_predictions = correct_predictions + 1\n",
        "  accuracy = correct_predictions / (len(testing)*64) # multiplying by 64 due to batch size\n",
        "  print(\"accuracy after DeepFool attack is {} or {}%\".format(accuracy, accuracy*100))\n",
        "test_with_DeepFool(model, device, testloader, atk3)\n"
      ],
      "metadata": {
        "id": "QlYc7LoNcU5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate adv image\n",
        "\n",
        "idx = 0\n",
        "pre = get_pred(model, adv_images[idx:idx+1], device)\n",
        "imshow(adv_images[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels1[idx], pre))"
      ],
      "metadata": {
        "id": "UW2XxDiMcZ0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "train_valloader = DataLoader(FASHION_trainval, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "NYiOG8bybUoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_train_Square = []\n",
        "def train_with_Square(model, device, training, attack):\n",
        "    # Loop over all examples in training set\n",
        "    for index, (batch, label) in enumerate(training):\n",
        "        if index == 8:\n",
        "          break\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        adv_image = attack(batch, label)\n",
        "        for index, Square_image in enumerate(adv_image):\n",
        "          \n",
        "          adv_train_Square.append((Square_image, label[index]))\n",
        "train_with_Square(model, device, train_valloader,atk3)  "
      ],
      "metadata": {
        "id": "Npzu_72XbyO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Create databse\n",
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "FASHION_trainval_Square = torch.utils.data.ConcatDataset([FASHION_trainval, adv_train_Square])\n",
        "train_len = int(0.8*len(FASHION_trainval_Square))\n",
        "FASHION_train_Square = Subset(FASHION_trainval_Square, range(train_len))\n",
        "FASHION_val_Square = Subset(FASHION_trainval_Square, range(train_len, len(FASHION_trainval_Square)))"
      ],
      "metadata": {
        "id": "K7znpeYdb1Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader_Square = DataLoader(FASHION_train_Square, batch_size=64, shuffle=True)\n",
        "valloader_Square = DataLoader(FASHION_val_Square, batch_size=64, shuffle=True)\n",
        "\n",
        "model_Square = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "optimizer = optim.Adam(model_Square.parameters(), lr=2*1e-3, weight_decay=7*1e-4)"
      ],
      "metadata": {
        "id": "NaO0FUyJb3lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_acc_epoch = []\n",
        "training_loss_epoch = []\n",
        "def train(model, loader, num_epoch = 10, model_name=\"model1\", validation=valloader): # Train the model\n",
        "    valid_acc_epoch = []\n",
        "    training_loss_epoch = []\n",
        "    print(\"Start training...\")\n",
        "    model.train() # Set the model to training mode\n",
        "    max_valid_acc = 0\n",
        "    for i in range(num_epoch):\n",
        "        running_loss = []\n",
        "        for batch, label in tqdm(loader):\n",
        "            batch = batch.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad() # Clear gradients from the previous iteration\n",
        "            pred = model(batch) # This will call Network.forward() that you implement\n",
        "            loss = criterion(pred, label) # Calculate the loss\n",
        "            running_loss.append(loss.item())\n",
        "            loss.backward() # Backprop gradients to all tensors in the network\n",
        "            optimizer.step() # Update trainable weights\n",
        "        print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
        "        valid_acc = evaluate(model, validation)\n",
        "        print(\"val acc = \",valid_acc)\n",
        "        valid_acc_epoch.append((i, valid_acc))\n",
        "        # print(\"line 2\")\n",
        "        training_loss_epoch.append((i, float(np.mean(running_loss))))\n",
        "        if valid_acc > max_valid_acc:\n",
        "          max_valid_acc = valid_acc\n",
        "          torch.save(model.state_dict(), model_name)\n",
        "        model.train()\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "a9EgpdYzb60M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    print(\"inside eval\")\n",
        "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
        "        print(\"between with and for\")\n",
        "        for batch, label in tqdm(loader):\n",
        "            # print(\"label = \",label)\n",
        "            print(len(batch))\n",
        "            batch = batch.to(device)\n",
        "            \n",
        "            # print(\"batch = \",batch)\n",
        "            label = label.to(device)\n",
        "            pred = model(batch)\n",
        "            # if label!=pred:\n",
        "            #   print(\"label = \",label)\n",
        "            #   print(\"pred = \",pred)\n",
        "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
        "    acc = correct/len(loader.dataset)\n",
        "    print(\"Evaluation accuracy: {}\".format(acc))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "ito8fsQHcAFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS THROWS ERROR"
      ],
      "metadata": {
        "id": "s9iQJxfbcD8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train(model_Square, trainloader_Square, 10, \"model_Square\", valloader_Square)"
      ],
      "metadata": {
        "id": "S2oHeyplcBvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carlini-Wagner"
      ],
      "metadata": {
        "id": "-c-Els2WcqDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def cw_attack(model, device, images, labels, targeted=False, num_classes=10, max_iterations=1000, learning_rate=0.01, c=1e-4, kappa=0):\n",
        "    # Convert images and labels to tensors and move to device\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Initialize the perturbation\n",
        "    batch_size = images.shape[0]\n",
        "    perturbation = torch.zeros_like(images, requires_grad=True)\n",
        "\n",
        "    # Initialize binary search parameters\n",
        "    low_confidence = 0\n",
        "    high_confidence = 1e10\n",
        "\n",
        "    # Define the loss function\n",
        "    def cw_loss_fn(outputs, labels, confidence):\n",
        "        # Calculate the logits for the target class\n",
        "        if targeted:\n",
        "            target_logits = outputs[:, labels]\n",
        "            other_logits = torch.max(outputs - target_logits.unsqueeze(1), dim=1)[0]\n",
        "        else:\n",
        "            target_logits = torch.max(outputs - 1e4*labels, dim=1)[0]\n",
        "            other_logits = torch.max(outputs - target_logits.unsqueeze(1), dim=1)[0]\n",
        "\n",
        "        # Calculate the margin loss\n",
        "        margin_loss = torch.clamp(other_logits - target_logits + kappa, min=0)\n",
        "        \n",
        "        # Calculate the L2 distance between the perturbation and the original image\n",
        "        l2_distance = torch.norm(perturbation.view(batch_size, -1), p=2, dim=1)\n",
        "        \n",
        "        # Calculate the final loss\n",
        "        loss = confidence*margin_loss.mean() + c*l2_distance.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam([perturbation], lr=learning_rate)\n",
        "\n",
        "    # Perform the attack\n",
        "    for i in range(max_iterations):\n",
        "        # Forward pass\n",
        "        outputs = model(images + perturbation)\n",
        "        outputs = outputs.max(1, keepdim=True)[1]\n",
        "        loss = cw_loss_fn(outputs, labels, high_confidence)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Project the perturbation onto the L-infinity ball of radius epsilon\n",
        "        perturbation.data = torch.clamp(perturbation.data, -1, 1)\n",
        "        perturbation.data *= torch.clamp((1.0 - 1e-10) * torch.ones_like(images) - torch.abs(images + perturbation.data - images), 0, 1)\n",
        "        # perturbation.data *= torch.clamp((1.0 - 1e-10) * torch.ones_like(torch.from_numpy(images)) - torch.abs(torch.from_numpy(images) + perturbation.data - torch.from_numpy(images)), 0, 1)\n",
        "        perturbation.grad.zero_()\n",
        "\n",
        "        # Check if the attack is successful\n",
        "        confidence = F.softmax(outputs.float(), dim=1)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        correct_predictions = (predicted_labels == labels)\n",
        "        correct_and_confident = correct_predictions & (confidence > 0.99)\n",
        "        success = correct_and_confident.sum().item() == 0\n",
        "\n",
        "        # Update the binary search parameters\n",
        "        if targeted:\n",
        "            if success:\n",
        "                high_confidence = confidence[~correct_predictions].min().item()\n",
        "            else:\n",
        "                low_confidence = confidence[correct_and_confident].max().item()\n",
        "        else:\n",
        "            if success:\n",
        "                low_confidence = confidence[correct_predictions].max().item()\n",
        "            else:\n",
        "                high_confidence = confidence[~correct_predictions].min().item()\n",
        "\n",
        "        # Check if the attack is successful and within the desired confidence level\n",
        "        if success and (high_confidence - low_confidence) < 1e-6:\n",
        "            break\n",
        "\n",
        "    # Return the perturbed images\n",
        "    adv_images = images + perturbation.detach()\n",
        "\n",
        "    return adv_images\n",
        "\n",
        "\n",
        "correct_predictions = 0\n",
        "for index, (batch, label) in enumerate(testloader):\n",
        "  batch = batch.to(device)\n",
        "  label = label.to(device)\n",
        "  if len(batch) != 64:\n",
        "    continue\n",
        "  batch_adv = cw_attack(model, device, batch, label)\n",
        "  output = model(batch_adv)\n",
        "  perturbed_prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "  perturbed_prediction_arr = torch.reshape(perturbed_prediction, (perturbed_prediction.size(dim=0), )).numpy()\n",
        "  label_arr = label.numpy()\n",
        "  for index in range(label.size(dim=0)):\n",
        "    if perturbed_prediction_arr[index] == label_arr[index]:\n",
        "        correct_predictions = correct_predictions + 1\n",
        "\n",
        "accuracy = correct_predictions / (len(testloader)*64) # multiplying by 64 due to batch size\n",
        "print(\"accuracy after CW attack is {} or {}%\".format(accuracy, accuracy*100))\n"
      ],
      "metadata": {
        "id": "z6kkj_YRclSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "\n",
        "train_valloader = DataLoader(FASHION_trainval, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "rbIQriGzc2HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_train_cw = []\n",
        "for index, (batch, label) in enumerate(train_valloader):\n",
        "  batch = batch.to(device)\n",
        "  label = label.to(device)\n",
        "  if index == 8:\n",
        "    break\n",
        "  batch_adv = cw_attack(model, device, batch, label)\n",
        "  for index, pgd_image in enumerate(batch):\n",
        "    adv_train_cw.append((pgd_image, label_arr[index]))"
      ],
      "metadata": {
        "id": "J3Szajs4c57z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Fashion* \n",
        "FASHION_trainval = datasets.FashionMNIST('.', download=True, train=True, transform=FASHION_transform)\n",
        "FASHION_test = datasets.FashionMNIST('.', download=True, train=False, transform=FASHION_transform)\n",
        "\n",
        "\n",
        "FASHION_trainval_cw = torch.utils.data.ConcatDataset([FASHION_trainval, adv_train_cw])\n",
        "train_len = int(0.8*len(FASHION_trainval_cw))\n",
        "FASHION_train_cw = Subset(FASHION_trainval_cw, range(train_len))\n",
        "FASHION_val_cw = Subset(FASHION_trainval_cw, range(train_len, len(FASHION_trainval_cw)))\n",
        "\n",
        "trainloader_cw = DataLoader(FASHION_train_cw, batch_size=64, shuffle=True)\n",
        "valloader_cw = DataLoader(FASHION_val_cw, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "model_cw = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "optimizer = optim.Adam(model_pgd.parameters(), lr=2*1e-3, weight_decay=7*1e-4)\n",
        "\n",
        "len(FASHION_trainval_cw)"
      ],
      "metadata": {
        "id": "Yr2FAcd1c8yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_cw, trainloader_cw, 10, \"model_cw\", valloader_cw)"
      ],
      "metadata": {
        "id": "tt0FDg3XdAlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}